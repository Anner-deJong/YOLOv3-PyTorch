{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_cfg(config_file):\n",
    "    \"\"\" \n",
    "    Parse a config file that states the network architecture.\n",
    "    Official Yolo v3 config can be found online and in ./cfg/yolov3.cfg\n",
    "    \n",
    "    Returns a list of dicts, each dict describing a layer in the network\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "\n",
    "    with open(config_file) as f:\n",
    "        # read in all lines, except for empty ones. :-1 is to skip the \\n charachter\n",
    "        lines = [line[:-1] for line in f if (line[:-1] != '')]\n",
    "        # FROM TUTORIAL:\n",
    "        lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
    "        lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
    "        \n",
    "        # skip through first non block lines\n",
    "        while (lines[0][0] != '['):\n",
    "            lines = lines[1:]\n",
    "        \n",
    "        # while the file is not empty, parse a new block\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            block = {}\n",
    "            block['type'] = lines[i][1:-1].rstrip()\n",
    "            i += 1\n",
    "            while (i < len(lines)) and (lines[i][0] != \"[\"):\n",
    "                key, val = lines[i].split('=')\n",
    "                block[key.rstrip()] = val.lstrip()             # rstrip() + lstrip() FROM TUTORIAL\n",
    "                i +=1\n",
    "            blocks.append(block)\n",
    "        \n",
    "        # seperate out first non-layer block (network meta info)\n",
    "        net_info = blocks[0]\n",
    "        \n",
    "    return net_info, blocks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_info, blocks = parse_cfg(\"./cfg/yolov3.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_modules(blocks):\n",
    "    # init\n",
    "    mod_list = nn.ModuleList()       # This list will contain all our layers\n",
    "    in_features = 3                  # Previous layer's # output channels (3 for RGB)\n",
    "    out_features = []                # keep track of each layers # output channels\n",
    "    \n",
    "    for idx, block in enumerate(blocks):\n",
    "        if block['type'] == 'convolutional':\n",
    "            mod_list.append(create_conv_layer(block, in_features, idx))\n",
    "            out_features.append(int(block['filters']))\n",
    "            in_features = out_features[-1]\n",
    "            \n",
    "        elif block['type'] == 'upsample':\n",
    "            mod_list.append(create_upsample_layer(block, idx))\n",
    "            out_features.append(out_features[-1])\n",
    "        \n",
    "        elif block['type'] == 'route':\n",
    "            mod_list.append(create_route_layer(block, idx))\n",
    "            out_feature = sum([out_features[int(i)] for i in block['layers'].split(',')])\n",
    "            out_features.append(out_feature)\n",
    "            in_features = out_features[-1]\n",
    "        \n",
    "        elif block['type'] == 'shortcut':\n",
    "            mod_list.append(create_shortcut_layer(block, idx))\n",
    "            out_features.append(out_features[-1])\n",
    "        \n",
    "        elif block['type'] == 'yolo':\n",
    "            mod_list.append(create_detection_layer(block, idx))\n",
    "            out_features.append(out_features[-1])\n",
    "    \n",
    "        else:\n",
    "            raise ValueError('Block type note recognised/implemented: {}'.format(block['type']))\n",
    "    \n",
    "    return mod_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list = create_modules(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    def __init__(self, config_file='./cfg_weights_utils/yolov3.cfg'):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.net_info, self.blocks = parse_cfg(config_file)\n",
    "        self.module_list           = create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, gpu_enabled=False):\n",
    "        \n",
    "        outputs = {}               # store all activation outputs to be accessed by routing and shortcuts\n",
    "        bboxes  = torch.empty(0)   # concatenate all bbox predictions to this initially empty tensor\n",
    "        \n",
    "        # process all modules in order\n",
    "        for i, module in enumerate(blocks):\n",
    "            \n",
    "            # convolutional or upsample layer\n",
    "            if module['type'] in ['convolutional', 'upsample']:\n",
    "                x = self.module_list[i](x)\n",
    "                outputs[i] = x\n",
    "            \n",
    "            # shortcut layer\n",
    "            elif module['type'] == 'shortcut':\n",
    "                frm = i + int(module['from'])\n",
    "                x.add_(outputs[frm])\n",
    "                outputs[i] = x\n",
    "            \n",
    "            # route layer\n",
    "            elif module['type'] == 'route':\n",
    "                layers = [int(layer) for layer in module['layers'].split(',')]     # get layers as int in a list\n",
    "                layers = [layer if (layer > 0) else i+layer for layer in layers]   # make all layers absolute\n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[layers[0]]\n",
    "                elif len(layers) == 2:\n",
    "                    x = torch.cat((outputs[layers[0]], outputs[layers[1]]), dim=1) # concatenate layers if more than 1\n",
    "                else:\n",
    "                    ValueError('Routing with more than 2 ({}) layers not implemented'.format(layers))\n",
    "                \n",
    "                outputs[i] = x\n",
    "            \n",
    "            # detection layer\n",
    "            elif module['type'] == 'yolo':\n",
    "                input_size  = int(self.net_info['width'])\n",
    "                anchors     = self.module_list[i][0].anchors\n",
    "                num_classes = int(module['classes'])\n",
    "                \n",
    "                preds       = predict_transform(x, input_size, anchors, num_classes)\n",
    "                bboxes      = torch.cat((bboxes, preds), dim=1)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError('Block type note recognised/implemented: {}'.format(block['type']))\n",
    "                \n",
    "        return bboxes\n",
    "    \n",
    "    def load_weights(self, weights_file='./cfg_weights_utils/yolov3.weights'):\n",
    "        # importing the weights seems extremely tedious.\n",
    "        # definitely couldn't have done it without the tutorial stating the order and datatype\n",
    "        # weights are stored in the order: batch_norm or bias, weights\n",
    "        # bn weights are stored in the order: bias, weight, running_mean, running_var\n",
    "        \n",
    "        with open(weights_file) as f:\n",
    "            # FROM TUTORIAL\n",
    "            _ = np.fromfile(f, dtype=np.int32, count=5)  # overhead\n",
    "            weights = np.fromfile(f, dtype=np.int32)     # all conv and batch norm weights\n",
    "        \n",
    "        \n",
    "        # check all modules in order\n",
    "        i_w = 0\n",
    "        for i_m, module in enumerate(blocks):\n",
    "            \n",
    "            # only conv layers carry weights\n",
    "            if module['type'] != 'convolutional':\n",
    "                continue\n",
    "            \n",
    "            # check whether conv layer includes batch norm, if not it contains a bias (below)\n",
    "            if int(module.get('batch_normalize', 0)):\n",
    "                # self.module_list[i_m][0] = conv layer\n",
    "                # self.module_list[i_m][1] = batch norm layer\n",
    "                \n",
    "                # batch norm bias\n",
    "                bn_bias_sh = self.module_list[i_m][1].bias.shape\n",
    "                bn_bias    = weights[i_w:i_w+np.prod(bn_bias)]\n",
    "                i_w       += np.prod(bn_bias_sh)\n",
    "                self.module_list[i_m][1].bias.data = torch.tensor(bn_bias).view(bn_bias_sh)\n",
    "                \n",
    "                total_w   += np.prod(self.module_list[i_m][1].weight.shape)\n",
    "                total_w   += np.prod(self.module_list[i_m][1].running_mean.shape)\n",
    "                total_w   += np.prod(self.module_list[i_m][1].running_var.shape)\n",
    "                # bn_w         = weights[i_w:i_w+np.prod(bn_w_shape)]\n",
    "                \n",
    "#                 self.module_list[i_m][1].weight.data = torch.tensor(bn_w).view(bn_w_shape)\n",
    "                \n",
    "                # self.module_list[i_m][0] = conv layer\n",
    "                total_w += np.prod(self.module_list[i_m][0].weight.shape)\n",
    "#                 conv_w       = weights[i_w:i_w+np.prod(conv_w_shape)]\n",
    "                # i_w         += np.prod(conv_w_shape)\n",
    "                # self.module_list[i_m][0].weight.data = torch.tensor(conv_w).view(conv_w_shape)\n",
    "\n",
    "            else:\n",
    "                # conv biases\n",
    "                total_w += np.prod(self.module_list[i_m][0].bias.shape)\n",
    "#                 conv_b       = weights[i_w:i_w+np.prod(conv_b_shape)]\n",
    "#                 i_w         += np.prod(conv_b_shape)\n",
    "#                 self.module_list[i_m][0].bias.data = torch.tensor(conv_b).view_as(self.module_list[i_m][0].bias)\n",
    "            \n",
    "                # conv weights\n",
    "                total_w += np.prod(self.module_list[i_m][0].weight.shape)\n",
    "#                 conv_w       = weights[i_w:i_w+np.prod(conv_w_shape)]\n",
    "#                 i_w         += np.prod(conv_w_shape)\n",
    "#                 self.module_list[i_m][0].weight.data = torch.tensor(conv_w).view(conv_w_shape)\n",
    "\n",
    "        print(total_w, len(weights))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62001757 62001757\n"
     ]
    }
   ],
   "source": [
    "darknet = Darknet('./cfg_weights_utils/yolov3.cfg')\n",
    "darknet.load_weights('./cfg_weights_utils/yolov3.weights')\n",
    "# weight file weights: 62001757\n",
    "# conv layer weights: 61922845\n",
    "# unaccounted for: 78912???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_all_buffers', '_apply', '_backend', '_backward_hooks', '_buffers', '_check_input_dim', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_modules', '_parameters', '_slow_forward', '_tracing_name', '_version', 'add_module', 'affine', 'apply', 'bias', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eps', 'eval', 'extra_repr', 'float', 'forward', 'half', 'load_state_dict', 'modules', 'momentum', 'named_children', 'named_modules', 'named_parameters', 'num_batches_tracked', 'num_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'reset_parameters', 'reset_running_stats', 'running_mean', 'running_var', 'share_memory', 'state_dict', 'to', 'track_running_stats', 'train', 'training', 'type', 'weight', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(np.prod(darknet.module_list[3][1].weight.data.shape))\n",
    "print(darknet.module_list[3][1].running_mean)\n",
    "print(dir(darknet.module_list[3][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62001757,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10647, 85])\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anner/Code/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/Users/anner/Code/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "out = darknet(torch.randn(1, 3, 32*13, 32*13))\n",
    "print(out.shape)\n",
    "print(out.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([5, 4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([6, 4])\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(0)\n",
    "print(a.shape)\n",
    "b = torch.ones(1, 4) * 2\n",
    "print(b.shape)\n",
    "c = torch.ones(5, 4) * 3\n",
    "print(c.shape)\n",
    "\n",
    "ab = torch.cat((a, b), 0)\n",
    "print(ab.shape)\n",
    "ab = torch.cat((ab, c), 0)\n",
    "print(ab.shape)\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = 0\n",
    "time2 = 0\n",
    "\n",
    "for _ in range(10000):\n",
    "    tic    = time.time()\n",
    "    pred   = predict_transform1(out, anchors=[1, 2, 3], num_classes=80)\n",
    "    time1 += time.time()-tic\n",
    "    \n",
    "    tic    = time.time()\n",
    "    pred   = predict_transform2(out, anchors=[1, 2, 3], num_classes=80)\n",
    "    time2 += time.time()-tic\n",
    "    \n",
    "print('elapsed time for 1: {:.2e}'.format(time1))\n",
    "print('elapsed time for 2: {:.2e}'.format(time2))\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 84 #84, 87\n",
    "print(blocks[n])\n",
    "layers = [int(layer) for layer in blocks[n]['layers'].split(',')]\n",
    "print(len(layers))\n",
    "a = np.arange(9)\n",
    "print(a[layers[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod_list[82][0].anchors)\n",
    "print(mod_list[94][0].anchors)\n",
    "print(mod_list[106][0].anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorstring = '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326'\n",
    "maskstring = '0,1,2'\n",
    "anchors = [anchor for anchor in anchorstring.split(', ')]\n",
    "print(anchors)\n",
    "mask    = [int(msk) for msk in maskstring.split(',')] #:  0,1,2\n",
    "anchors = [list(map(int, anchors[i].split(','))) for i in mask]\n",
    "print(anchors)\n",
    "# anchors = [[anchors[i], anchors[i+1]] for i in range(0, len(anchors), 2)]\n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -12\n",
    "\n",
    "print(blocks[n]['type'])\n",
    "conv_block = blocks[n]\n",
    "for key, value in conv_block.items():\n",
    "    print(key, ': ', value)\n",
    "\n",
    "    \n",
    "print([int(i) for i in conv_block['layers'].split(',')])\n",
    "# out_feature = sum([out_features[int(i)] for i in conv_block['layers'].split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blocks[9])\n",
    "print(len(blocks))\n",
    "for i, item in enumerate(blocks[1:]):\n",
    "    print(i, item['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(9).reshape(3,3)\n",
    "# x = x.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x)\n",
    "print(x.dtype)\n",
    "y = torch.ones(3, 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = nn.Sequential(\n",
    "    nn.Linear(4, 5),\n",
    "    nn.Linear(5, 6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.eye(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(x.type(torch.float))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.matmul(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
